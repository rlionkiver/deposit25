{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c27b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Training] Semua data lengkap, gak ada yang kosong.\n",
      "\n",
      "=== Evaluasi Model Individu ===\n",
      "\n",
      "Training LGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil evaluasi LGBM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      3046\n",
      "           1       0.58      0.28      0.38       392\n",
      "\n",
      "    accuracy                           0.90      3438\n",
      "   macro avg       0.75      0.63      0.66      3438\n",
      "weighted avg       0.88      0.90      0.88      3438\n",
      "\n",
      "AUC: 0.7951\n",
      "\n",
      "Training RandomForest...\n",
      "\n",
      "Hasil evaluasi RandomForest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      3046\n",
      "           1       0.47      0.59      0.52       392\n",
      "\n",
      "    accuracy                           0.88      3438\n",
      "   macro avg       0.71      0.75      0.73      3438\n",
      "weighted avg       0.89      0.88      0.88      3438\n",
      "\n",
      "AUC: 0.7929\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil evaluasi XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94      3046\n",
      "           1       0.58      0.31      0.40       392\n",
      "\n",
      "    accuracy                           0.90      3438\n",
      "   macro avg       0.75      0.64      0.67      3438\n",
      "weighted avg       0.88      0.90      0.88      3438\n",
      "\n",
      "AUC: 0.7793\n",
      "\n",
      "Training CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil evaluasi CatBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95      3046\n",
      "           1       0.63      0.29      0.40       392\n",
      "\n",
      "    accuracy                           0.90      3438\n",
      "   macro avg       0.77      0.63      0.67      3438\n",
      "weighted avg       0.88      0.90      0.88      3438\n",
      "\n",
      "AUC: 0.7863\n",
      "\n",
      "=== Membuat Stacking Model ===\n",
      "Training stacking model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil evaluasi Stacking Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94      3046\n",
      "           1       0.50      0.44      0.47       392\n",
      "\n",
      "    accuracy                           0.89      3438\n",
      "   macro avg       0.72      0.69      0.70      3438\n",
      "weighted avg       0.88      0.89      0.88      3438\n",
      "\n",
      "AUC: 0.7708\n",
      "\n",
      "=== Membuat Voting Model ===\n",
      "Training voting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil evaluasi Voting Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      3046\n",
      "           1       0.56      0.35      0.43       392\n",
      "\n",
      "    accuracy                           0.89      3438\n",
      "   macro avg       0.74      0.66      0.68      3438\n",
      "weighted avg       0.88      0.89      0.88      3438\n",
      "\n",
      "AUC: 0.7953\n",
      "\n",
      "Tidak ada model yang mencapai AUC >= 0.80, menggunakan Stacking Model terbaik\n",
      "\n",
      "Model terbaik disimpan di 'deposito_predictor.pkl'\n",
      "Gagal bikin plot fitur penting: 'StackingClassifier' object has no attribute 'feature_importances_'\n",
      "Gagal generate penjelasan SHAP: Model type not yet supported by TreeExplainer: <class 'sklearn.ensemble._stacking.StackingClassifier'>\n",
      "\n",
      "File submission berhasil dibuat di 'submission.csv'\n",
      "\n",
      "Preview hasil submission:\n",
      "   customer_number  berlangganan_deposito\n",
      "0           907098               0.043122\n",
      "1           699895               0.070934\n",
      "2           440407               0.099262\n",
      "3           787412               0.234866\n",
      "4           891667               0.040041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RLION\\anaconda3\\envs\\TF\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 1. Load data and check data\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def check_missing_values(df, name=\"Dataset\"):\n",
    "    missing = df.isnull().sum()\n",
    "    total_missing = missing.sum()\n",
    "    if total_missing > 0:\n",
    "        print(f\"[{name}] Ada data yang kosong:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(f\"[{name}] Semua data lengkap, gak ada yang kosong.\")\n",
    "\n",
    "# 2. Feature engineering with original column names\n",
    "class BusinessFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # 1. Fitur kapasitas finansial\n",
    "        if all(col in X.columns for col in ['balance', 'income']):\n",
    "            X['savings_ratio'] = X['balance'] / (X['income'] + 1)\n",
    "            X['balance_income_interaction'] = X['balance'] * X['income']\n",
    "            X['financial_stability'] = np.log1p(X['balance']) - np.log1p(X['income'])\n",
    "            X['income_to_age_ratio'] = X['income'] / (X['age'] + 1)\n",
    "\n",
    "        # 2. Fitur kelompok umur\n",
    "        if 'age' in X.columns:\n",
    "            bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "            labels = ['18-24', '25-34', '35-44', '45-54', '55-64', '65+']\n",
    "            X['age_group'] = pd.cut(X['age'], bins=bins, labels=labels)\n",
    "            X['age_squared'] = X['age'] ** 2\n",
    "            X['is_senior'] = (X['age'] >= 60).astype(int)\n",
    "\n",
    "        # 3. Fitur engagement kampanye\n",
    "        if all(col in X.columns for col in ['previous_campaign_contacts', 'previous_campaign_success']):\n",
    "            X['engagement_intensity'] = np.where(\n",
    "                X['previous_campaign_contacts'] > 0,\n",
    "                X['previous_campaign_success'] / X['previous_campaign_contacts'],\n",
    "                0\n",
    "            )\n",
    "            X['high_engagement'] = (X['previous_campaign_contacts'] > np.percentile(\n",
    "                X['previous_campaign_contacts'], 75)).astype(int)\n",
    "            X['responsive_client'] = (X['engagement_intensity'] > 0.5).astype(int)\n",
    "            X['campaign_frequency'] = np.log1p(X['previous_campaign_contacts'])\n",
    "\n",
    "        # 4. Fitur durasi kontak\n",
    "        if 'last_contact_duration' in X.columns:\n",
    "            X['contact_efficiency'] = np.log1p(X['last_contact_duration']) / (\n",
    "                np.log1p(X['previous_campaign_contacts']) + 1)\n",
    "            X['duration_bins'] = pd.qcut(X['last_contact_duration'], 5, labels=False, duplicates='drop')\n",
    "            X['duration_per_contact'] = X['last_contact_duration'] / (X['previous_campaign_contacts'] + 1)\n",
    "\n",
    "        # 5. Fitur interaksi\n",
    "        if all(col in X.columns for col in ['age', 'balance']):\n",
    "            X['age_balance_interaction'] = X['age'] * X['balance']\n",
    "            X['age_income_interaction'] = X['age'] * X['income']\n",
    "\n",
    "        # 6. Fitur balance\n",
    "        if 'balance' in X.columns:\n",
    "            X['balance_squared'] = X['balance'] ** 2\n",
    "            X['balance_log'] = np.log1p(X['balance'])\n",
    "            X['balance_per_age'] = X['balance'] / (X['age'] + 1)\n",
    "\n",
    "        # 7. Fitur customer tenure\n",
    "        if 'customer_since' in X.columns:\n",
    "            X['customer_since'] = pd.to_datetime(X['customer_since'], errors='coerce')\n",
    "            X['customer_tenure'] = 2023 - X['customer_since'].dt.year\n",
    "            X['tenure_group'] = pd.cut(X['customer_tenure'], bins=[0, 2, 5, 10, 20, 100],\n",
    "                                      labels=['0-2', '3-5', '6-10', '11-20', '20+'])\n",
    "\n",
    "        return X\n",
    "\n",
    "# 3. Model interpretation\n",
    "def plot_feature_importance(model, X_val):\n",
    "    try:\n",
    "        if hasattr(model.named_steps['preprocessor'], 'get_feature_names_out'):\n",
    "            feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "        else:\n",
    "            feature_names = X_val.columns.tolist()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        importances = model.named_steps['classifier'].feature_importances_\n",
    "        indices = np.argsort(importances)[-25:]\n",
    "        plt.title('Fitur Paling Penting')\n",
    "        plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Pentingnya Fitur (Relatif)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"\\nPlot fitur penting udah disimpan di 'feature_importance.png'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal bikin plot fitur penting: {str(e)}\")\n",
    "\n",
    "def explain_model(model, X_val, sample_idx=0):\n",
    "    try:\n",
    "        business_features = model.named_steps['business_features'].transform(X_val)\n",
    "        preprocessed_data = model.named_steps['preprocessor'].transform(business_features)\n",
    "\n",
    "        explainer = shap.TreeExplainer(model.named_steps['classifier'])\n",
    "        shap_values = explainer.shap_values(preprocessed_data)\n",
    "\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, preprocessed_data,\n",
    "                         feature_names=model.named_steps['preprocessor'].get_feature_names_out(),\n",
    "                         plot_type=\"bar\", show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"Plot SHAP summary udah disimpan di 'shap_summary.png'\")\n",
    "\n",
    "        shap.force_plot(explainer.expected_value, shap_values[0][sample_idx, :],\n",
    "                        preprocessed_data[sample_idx, :],\n",
    "                        feature_names=model.named_steps['preprocessor'].get_feature_names_out(),\n",
    "                        matplotlib=True, show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_force_plot.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"Plot SHAP force udah disimpen di 'shap_force_plot.png'\")\n",
    "\n",
    "        plt.figure()\n",
    "        shap.decision_plot(explainer.expected_value, shap_values[0][:100, :],\n",
    "                          feature_names=model.named_steps['preprocessor'].get_feature_names_out(),\n",
    "                          show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_decision_plot.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"Plot SHAP decision udah disimpen di 'shap_decision_plot.png'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Gagal generate penjelasan SHAP: {str(e)}\")\n",
    "\n",
    "# 4. Generate submission file\n",
    "def generate_submission_file(model, X_val, original_val_data):\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        submission_df = pd.DataFrame({\n",
    "            'customer_number': original_val_data['customer_number'].values,\n",
    "            'berlangganan_deposito': y_proba\n",
    "        })\n",
    "        \n",
    "        submission_df = submission_df[['customer_number', 'berlangganan_deposito']]\n",
    "        submission_df.to_csv('submission.csv', index=False)\n",
    "        print(\"\\nFile submission berhasil dibuat di 'submission.csv'\")\n",
    "        print(\"\\nPreview hasil submission:\")\n",
    "        print(submission_df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Gagal membuat file submission: {str(e)}\")\n",
    "\n",
    "# 5. Train and evaluate multiple models\n",
    "def train_and_evaluate():\n",
    "    # Load data\n",
    "    train_df = load_data(\"../training_dataset.csv\")\n",
    "    check_missing_values(train_df, \"Data Training\")\n",
    "\n",
    "    # Split features and target\n",
    "    X = train_df.drop(columns=['berlangganan_deposito', 'customer_number'])\n",
    "    y = train_df['berlangganan_deposito']\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=42, stratify=y\n",
    "    )\n",
    "    original_val_data = train_df.iloc[X_val.index].copy()\n",
    "\n",
    "    # Define numeric and categorical columns\n",
    "    num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    # Preprocessor\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "    ])\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'LGBM': LGBMClassifier(\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1\n",
    "        ),\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='auc',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        ),\n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            iterations=500,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            eval_metric='AUC',\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # SMOTE for imbalance\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Evaluate each model individually\n",
    "    print(\"\\n=== Evaluasi Model Individu ===\")\n",
    "    best_model = None\n",
    "    best_auc = 0\n",
    "    best_model_name = \"\"\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        pipeline = ImbPipeline(steps=[\n",
    "            ('business_features', BusinessFeatureTransformer()),\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('smote', smote),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, y_proba)\n",
    "        \n",
    "        print(f\"\\nHasil evaluasi {name}:\")\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        print(f\"AUC: {auc:.4f}\")\n",
    "        \n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_model = pipeline\n",
    "            best_model_name = name\n",
    "\n",
    "    # Stacking model\n",
    "    print(\"\\n=== Membuat Stacking Model ===\")\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=list(models.items()),\n",
    "        final_estimator=LogisticRegression(max_iter=1000),\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        passthrough=True\n",
    "    )\n",
    "\n",
    "    stacking_pipeline = ImbPipeline(steps=[\n",
    "        ('business_features', BusinessFeatureTransformer()),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', smote),\n",
    "        ('classifier', stacking_model)\n",
    "    ])\n",
    "\n",
    "    print(\"Training stacking model...\")\n",
    "    stacking_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_stack = stacking_pipeline.predict(X_val)\n",
    "    y_proba_stack = stacking_pipeline.predict_proba(X_val)[:, 1]\n",
    "    auc_stack = roc_auc_score(y_val, y_proba_stack)\n",
    "    \n",
    "    print(\"\\nHasil evaluasi Stacking Model:\")\n",
    "    print(classification_report(y_val, y_pred_stack))\n",
    "    print(f\"AUC: {auc_stack:.4f}\")\n",
    "\n",
    "    # Voting classifier\n",
    "    print(\"\\n=== Membuat Voting Model ===\")\n",
    "    voting_model = VotingClassifier(\n",
    "        estimators=list(models.items()),\n",
    "        voting='soft',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    voting_pipeline = ImbPipeline(steps=[\n",
    "        ('business_features', BusinessFeatureTransformer()),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', smote),\n",
    "        ('classifier', voting_model)\n",
    "    ])\n",
    "\n",
    "    print(\"Training voting model...\")\n",
    "    voting_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_vote = voting_pipeline.predict(X_val)\n",
    "    y_proba_vote = voting_pipeline.predict_proba(X_val)[:, 1]\n",
    "    auc_vote = roc_auc_score(y_val, y_proba_vote)\n",
    "    \n",
    "    print(\"\\nHasil evaluasi Voting Model:\")\n",
    "    print(classification_report(y_val, y_pred_vote))\n",
    "    print(f\"AUC: {auc_vote:.4f}\")\n",
    "\n",
    "    # Select best model\n",
    "    final_model = None\n",
    "    if auc_stack >= 0.80:\n",
    "        final_model = stacking_pipeline\n",
    "        print(\"\\nMenggunakan Stacking Model (AUC >= 0.80)\")\n",
    "    elif auc_vote >= 0.80:\n",
    "        final_model = voting_pipeline\n",
    "        print(\"\\nMenggunakan Voting Model (AUC >= 0.80)\")\n",
    "    elif best_auc >= 0.80:\n",
    "        print(f\"\\nMenggunakan {best_model_name} Model (AUC >= 0.80)\")\n",
    "        final_model = best_model\n",
    "    else:\n",
    "        final_model = stacking_pipeline\n",
    "        print(\"\\nTidak ada model yang mencapai AUC >= 0.80, menggunakan Stacking Model terbaik\")\n",
    "\n",
    "    # Save model\n",
    "    with open(\"deposito_predictor.pkl\", \"wb\") as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    print(\"\\nModel terbaik disimpan di 'deposito_predictor.pkl'\")\n",
    "\n",
    "    # Feature importance and SHAP\n",
    "    plot_feature_importance(final_model, X_val)\n",
    "    explain_model(final_model, X_val)\n",
    "    \n",
    "    # Generate submission\n",
    "    generate_submission_file(final_model, X_val, original_val_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7117e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
